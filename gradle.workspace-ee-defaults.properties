###
### DO NOT MODIFY THIS FILE in your workspace project.
###
### Instead copy the project property into 'gradle.properties' and set your
### custom value there. Or use init script / command line where appropriate.
###
### This file defines the configuration properties used by Liferay Workspace EE and
### their default values. You can check how the properties are being read by the build
### in script /gradle/liferay-workspace-ee/liferay-workspace-ee.gradle.w
###

###
### Properties which should be specified _in Gradle init script_ and passed to Gradle
### when running the build
###

# Please check file 'init.workspace-ee-sample.gradle' for complete list of supported
# properties and guidelines how to pass them to your Gradle build.

# Note that although you can specify all of the init-script properties on command line
# as well (-P...), it's strongly discouraged. Many of the properties in init script
# contain sensitive values (security credentials to other systems) and since the commands
# will show up in list of running processes to all other users in your OS and also
# stay in your e.g. Bash history, it would be a severe security hole.


###
### Properties which should be specified _on Gradle command line_ when running the build
###


# With what "build number" should the Liferay workspace EE build work? This is
# typically a counter managed outside of source code, in running system like Jenkins.
# Gradle will use this number to name
# You will typically want to define build of workspace using Jenkins as:
#
#   ./gradlew distBundleZip distBundleTar distBundleDeb distBundleRpm
#           -Pliferay.workspace.environment=dev
#			-PreleaseNumber=$BUILD_NUMBER
#           --no-daemon
#
# BUILD_NUMBER is a standard variable populated by Jenkins inside every job, which
# then can be used in definition of build steps.
#
releaseNumber=1


###
### Properties which should go into _gradle.properties_, if custom value is needed
###

# Comma-separated list of environments to use in the build. This overrides
# the property 'liferay.workspace.environment' IF given task supports it. The values
# are validated against the directories in /config - only existing directories can
# be used, to catch typos is 'prod' vs. 'prud'.
#
# Some jobs may benefit from multiple environments being involved in the build,
# so this property is the extension of 'liferay.workspace.environment'. The individual
# tasks have to correctly decide what to use. Tasks currently using this property:
#   * removeProjectAmisBeyondRetention
#   * removeProjectAmisBeyondRetentionDryRun
#
liferay.workspace.ee.environments=

# What to use as the source for determining the 'projectVersion' when naming
# the built artifacts? This will be made available in the Groovy code as
# 'gradle.liferayWorkspaceEe.projectVersion' once the subproject :liferay-workspace-ee
# is evaluated (note: this happens AFTER the root project is evaluated).
#
# Possible values are:
#   * gradle
#       * the version will be taken directly from Gradle
#       * build.gradle -> project.version
#       * '0.0.0' will be used if not defined
#
#   * git-branch
#       * the version will be the name of the currently checked out branch in git, lower-cased
#       * note - if branch could not be determined (e.g. the directory is not a git repository,
#               or any other exception) then the version will fall back to 'gradle', see above
#
#
# NOTE: If you use 'git-branch', the version is used to build .deb / .rpm (note:
# .deb is used to build AMI). So if you use git-branch, make sure all the branches
# you will be building have correct version-like names.
#
# For example, the Gradle plugin building .deb enforces this rule:
#   "Invalid upstream version 'gsms.326.deploy.updates' - a valid version must start
#       with a digit and only contain [A-Za-z0-9.+:~-]"
#
# When building .deb / .rpm, the version is sanitized. One of the steps is to make sure
# the version starts with a digit. So for example, by building 'master', you will
# end up with a version '1.master' if git-branch is used.
#
liferay.workspace.ee.project.version.source=gradle
#liferay.workspace.ee.project.version.source=git-branch

# The major version of the JDK for this workspace project. It is:
#
#  (1) checked to be used when starting the build (using Gradle) and
#  (2) Oracle JDK of this major version is when building OS packages (.deb and .rpm)
#       and 'liferay.workspace.ee.ospackage.jdk.installation.type' is either 'oracle-jdk:bundled'
#       or 'oracle-jdk:downloaded-on-install'
#
# The only allowed values are 7 and 8. Defaults to 8, which fits DXP / Liferay 7.0.
# For Liferay 6.2, use 7.
#
# Note: this does not necessarily apply to plugins-sdk, if used. There the detection
# is done using Ant.
#
liferay.workspace.ee.java.version.major=8

##
## The settings of the bundle being used
##

# The type of Liferay bundle you are using, referring to app server being used.
# Following ones are currently supported:
#   - tomcat
#
# This has to be matching the bundle that is specified in 'liferay.workspace.bundle.url'
# inside gradle.properties, which should default to Tomcat bundle URL like:
#   https://sourceforge.net/projects/lportal/files/Liferay Portal/7.0.0 GA1/liferay-portal-tomcat-7.0-ce-ga1-20160331161017956.zip
#
liferay.workspace.ee.bundle.type=tomcat
liferay.workspace.ee.bundle.app.server.directory.name=tomcat-8.0.32

# The SHA-256 checksum of the bundle used for the build (liferay.workspace.bundle.url).
# You only need to provide the value if you are using non-standard bundle - we have
# pre-computed checksums of known known bundles (keyed by the bundle's file name).
#
liferay.workspace.ee.bundle.sha256.checksum=

#liferay.workspace.ee.bundle.sha256.checksum=89ba6c75d86386c13ad24ac297768125c914de387f46279d461f7e8801884174

# Comma-separated list of files (or directories) removed from the bundle when extracted.
# Useful to exclude files which we know are in the bundle, but we will not need
# them in our project. For example:
#   * large patch files (we might be installing newer fixpack in our build, making the default one not used)
#   * .lpkg file not needed
#   * some pre-installed webapps (for Liferay 6.2)
#
# NOTE: Specified files have to be present in the bundle, otherwise the build will fail.
# This check is perform to prevent typos in the file's names / paths, causing the actual
# files not being excluded as expected.
#
liferay.workspace.ee.bundle.exclude.files=

#liferay.workspace.ee.bundle.exclude.files=\
#  patching-tool/patches/liferay-fix-pack-de-7-7010.zip,\
#  osgi/marketplace/Liferay Documentum Connector.lpkg,\
#  osgi/marketplace/Liferay OAuth Provider.lpkg
#  tomcat-7.0.62/webapps/calendar-portlet

# If set to 'true', the bundle installed using .deb / .rpm will have tighter
# file permissions set before being started with /etc/init.d script. Please note
# that some developer features might not be available afterwards (for example hot-deploy,
# OSGi auto-deploy etc.)
#
liferay.workspace.ee.bundle.harden.on.start=false

# If set to true, then the NPM cache of all modules won't be placed in the project
# directory of every theme, but rather stored and re-used from user's home directory.
# Cache directory will then be:
#   ~/.liferay/node-modules-cache
#
# WARNING: This is an experimental feature. Do not use parallel Gradle builds when
#           global npm cache is enabled.
#
#           Instead, you can use external Node JS installation (installed on the machine
#           where you run the build), with setting `node.download = false`.
#           For details, please see documentation:
#               https://dev.liferay.com/develop/reference/-/knowledge_base/7-0/node-gradle-plugin#project-extension
#
#
liferay.workspace.ee.persist.npm.node.modules.cache=false


##
## Patches installed into the bundle when built (initBundle / distBundle* tasks of Workspace plugin)
##

# The base URL to download the patching tool from. This must be an URL starting
# with https://files.liferay.com.
#
liferay.workspace.ee.patches.patching.tool.download.base.url=https://files.liferay.com/private/ee/fix-packs/patching-tool

# The file name of the patching tool to install, must be downloadable from
# 'liferay.workspace.ee.patches.patching.tool.download.base.url' and must be pointing
# to a valid patching tool's .zip archive.
#
# You can leave this empty and the patching tool included in the base bundle
# (liferay.workspace.bundle.url) will be used.
#
# NOTE: You will need to provide credentials the first time you are downloading a new version
#       of the tool. Then the downloaded .zip file is cached in ~./liferay/patching-tool
#       directory on your machine.
#
liferay.workspace.ee.patches.patching.tool.installed=

#liferay.workspace.ee.patches.patching.tool.installed=patching-tool-2.0.7.zip
#liferay.workspace.ee.patches.patching.tool.installed=patching-tool-2.0.7-internal.zip


# The base URL from where all downloaded patches (liferay.workspace.ee.patches.download.and.install.*)
# will be fetched. For DXP, you should never need to alter this URL, for 6.2 you my want to use something
# like 'https://files.liferay.com/private/ee/fix-packs/6.2.10'.
#
# NOTE: You cannot fetch from anywhere else than https://files.liferay.com/*, you can only alter
#       the path on the server.
#
liferay.workspace.ee.patches.download.base.url=https://files.liferay.com/private/ee/fix-packs/7.0.10

# Comma-separated list of paths to patches (fixpacks & hotfixes) which should get installed
# into the bundle. These paths have to be downloadable under the base URL
# 'liferay.workspace.ee.patches.download.base.url', e.g. in one of the subdirectories.
#
# Fixpacks for DXP are in subdirectory 'de/', hotfixes in 'hotfix/':
#   * de/liferay-fix-pack-de-10-7010.zip
#           -> final URL https://files.liferay.com/private/ee/fix-packs/7.0.10/de/liferay-fix-pack-de-10-7010.zip
#   * hotfix/liferay-hotfix-7-7010.zip
#           -> final URL https://files.liferay.com/private/ee/fix-packs/7.0.10/hotfix/liferay-hotfix-7-7010.zip
#
# Credentials are required (files.liferay.com), but the downloads are cached afterwards (~/.liferay/patches).
#
liferay.workspace.ee.patches.download.and.install.patches=

#liferay.workspace.ee.patches.download.and.install.patches=\
#	de/liferay-fix-pack-de-10-7010.zip,\
#	hotfix/liferay-hotfix-7-7010.zip,\
#	hotfix/liferay-hotfix-35-7010.zip,


# The root-level directory containing any custom patches for the project which should be
# installed as well. All *.zip files will be fetched and placed into 'patching-tool/patches'
# of the built bundle, together will all downloaded patches.
#
# Note: Use 'liferay.workspace.ee.patches.download.and.install.patches' as much as possible
# to avoid committing large files into source control.
#
liferay.workspace.ee.patches.dir=patches


##
## Jenkins management
##

# The URL where your project's Jenkins server could be reached by the project's
# workspace scripts.
#
# Credentials should always be passed on command line, using init script, see
# 'liferay.workspace.ee.jenkins.server.secure'.
#
# Notes:
#  (1) It's strongly encouraged to only use HTTPS when your Jenkins in not running locally
#      (on localhost or in secure internal network). If you'd use HTTP for Jenkisn running
#      remotely, your credentials would be transferred over the network in plain text -
#      Basic HTTP auth will be used, where credentials are only Base64 encoded and
#      anyone can easily decode them and steal your identity.
#
#  (2) If using HTTPS, the certificate returned by the Jenkins server has to be
#      a fully valid one to form an SSL connection. This especially means (but is
#      not limited to) that the certificate:
#       * has to be publicly trusted (not self-signed or signed by some untrusted authority)
#       * has to match the domain used for the HTTPS request
#       * cannot be expired, not yet valid, revoked etc.
#
#  (3) You can leverage SSH tunnel with local port forwarding to use safe communication
#      with Jenkins over HTTP if you are not able to satisfy (1) or (2) above.
#      Example:
#           ssh -i lfrgs-liferay-jarvis-jenkins.pem -N -L 18080:localhost:8080 ec2-user@ec2-54-165-239-234.compute-1.amazonaws.com
#
#      In this example, we will forward any traffic on local port 18080 to localhost:8080
#      as if accessed from ec2-54-165-239-234.compute-1.amazonaws.com. -N suppresses the login
#      over SSH, the process will stay running in the foreground, only forwarding the port.
#
#      As an authentication for SSH, user 'ec2-user' is specified and instead of a password,
#      we are providing a private SSH key file lfrgs-liferay-jarvis-jenkins.pem (e.g. from
#      AWS key pair). You can use any type of authentication for SSH, assuming the SSHD
#      on your Jenkins server supports it.
#
liferay.workspace.ee.jenkins.server.url=http://localhost:18080

# Configure whether the Jenkins server running on URL 'liferay.workspace.ee.jenkins.server.url'
# requires credentials (username + password) or not. If set to 'true', you need to provide
# 'jenkinsUserName' and 'jenkinsPassword', either one by one on command line (gradlew -P...)
# or through init script:
#   * 'jenkinsUserName' = the login for Jenkins REST API
#   * 'jenkinsPassword'	= the password (or API token) to use for the Jenkins REST API
#
# NOTE: Using -P will leave your credentials on shell (e.g. Bash) history, viewable
#       to anyone on your OS. We strongly encourage use of init scripts. See
#       init.workspace-ee-sample.gradle / init.workspace-ee-sample-encrypted.gradle
#
# NOTE: Some actions, like restarting Jenkins remotely (may be triggered by the scripts
#       if new Jenkins plugins were installed) always require credentials with admin
#       privileges in Jenkins. This is true even if using various "X users can do anything"
#       settings in Jenkins security.
#
# WARNING: Use 'false' only for local testing and never in actual project's CI server.
#
liferay.workspace.ee.jenkins.server.secure=true

# The directory, relative to this file, where jobs and views from remote Jenkins server will
# be stored when performing backup. Also these jobs and views will be pushed to Jenkins when
# performing Jenkins restore. Initial set of jobs will be provided if this directory
# does not exist.
#
liferay.workspace.ee.jenkins.items.dir=jenkins

# Comma-separated list of Jenkins job names which will be managed by the workspace.
# This is useful if you have created a job on your Jenkins server and you want to
# back it up into your workspace.
#
# All jobs backed up under 'liferay.workspace.ee.jenkins.items.dir'/jobs will be
# automatically managed as well.
#
# The default 'nnn' jobs are samples which can be created on the Jenkins server
# to create jobs for individual environments, like DEV.
#
liferay.workspace.ee.jenkins.managed.job.names=\
    build-nnn,\
    deploy-nnn,\
    build-nnn_ami,\
    deploy-nnn_ami

# Comma-separated list of Jenkins view names which will be managed by the workspace.
# This is useful if you have created a view on your Jenkins server and you want to
# back it up into your workspace.
#
# All view backed up under 'liferay.workspace.ee.jenkins.items.dir'/views will be
# automatically managed as well.
#
# The default 'Samples' view will group all the 'nnn' sample jobs, as listed in
# 'liferay.workspace.ee.jenkins.managed.job.names'.
#
liferay.workspace.ee.jenkins.managed.view.names=\
    Samples

# Comma-separated list of Jenkins plugins (<name>@<version>) which will get installed
# when using tasks to initialize Jenkins server. The ones listed by default are
# the ones which are used by sample jobs.
#
# Brief plugins' description:
# * credentials-binding@latest
#       Provide secret files to builds (as Gradle init scripts), with sensitive data,
#       like credentials to Nexus / files.liferay.com
# * config-file-provider@latest
#       provide plain-text files to builds (as Gradle init scripts), with non-sensitive data,
#       like location of Packer binary on file system of Jenkins VM
# * envinject@latest
#       To allow to read and set Jenkins variables from a properties file;
#       we can write this file using Shell build step (e.g. reading AMI ID from .txt file)
# * jenkins-cloudformation-plugin@latest
#       To build / destroy Cloud Formation stacks in jobs, based on CF template file + inputs
# * git@latest
#       To allow to fetch sources using git, e.g. from GitHub (SCM)
# * github@latest
#       Some extra integration with GitHub
# * gradle@latest
#       To run gradle wrapper as build step
# * copyartifact@latest
#       To build artifacts in one job, deploy in second
# * publish-over-ssh@latest
#       To push ZIP (DEB, RPM...) over SSH to remote server (using SCP)
# * build-name-setter
#       To be able to set user-friendly name / description of a job's run (build)
# * git-parameter
#       To be able to get a user-friendly selector of available branches to choose from for build-* jobs
# * conditional-buildstep
#       To be able to define jobs which execute differently based on their params
# * ansicolor
#       To be able to have nicely colored output, since the build tools we use (Gradle, Packer, Ansible)
#       often produce output with colored lines using ANSI escape sequences.
#
liferay.workspace.ee.jenkins.installed.plugin.ids=\
    credentials-binding@latest,\
    config-file-provider@latest,\
    envinject@latest,\
    jenkins-cloudformation-plugin@latest,\
    git@latest,\
    github@latest,\
    gradle@latest,\
    copyartifact@latest,\
    publish-over-ssh@latest,\
    build-name-setter@latest,\
    git-parameter@latest,\
    conditional-buildstep@latest,\
    ansicolor@latest


##
## Configuration for baking new AMIs in AWS for every new build (e.g. 'gradlew distBundleAmi')
##

# The target region where this project AMIs will be created and stored. The temporary
# EC2 instance used to create the AMI contents will be also launched in this region.
#
# Pricing differs between regions, see https://aws.amazon.com/ec2/pricing/.
#
liferay.workspace.ee.aws.ami.primary.region=us-east-1

# The ID of the AMI on which to base the project builds. Make sure that the region
# is the same as the target AMI region ('liferay.workspace.ee.aws.ami.primary.region'),
# otherwise the base AMI won't be found and the project's AMI won't not be created.
#
# To lookup latest Ubuntu AMIs, use e.g. https://cloud-images.ubuntu.com/locator/ec2/
#   * ami-5aa69030 ~ Ubuntu 14.04 LTS, amd64, hvm:ebs-ssd (us-east-1)
#   * ami-766d771a ~ Ubuntu 14.04 LTS, amd64, hvm:ebs-ssd (eu-central-1)
#
# To lookup latest CentOS images, use e.g. https://wiki.centos.org/Cloud/AWS
#	* ami-57cd8732 ~ CentOS 6 (x86_64) - with Updates HVM (us-east-1)
#	* ami-2a868b37 ~ CentOS 6 (x86_64) - with Updates HVM (eu-central-1)
#
liferay.workspace.ee.aws.ami.base.ami.id=ami-5aa69030

# The packaging format in the Linux OS installed in the used base AMI (see 'aws.ami.base.ami.id').
# It has to be either 'deb' or 'rpm' and determines which packaging mechanism will
# be used to install Liferay bundle into the base AMI.
#   * 'deb' ~ Debian, Ubuntu etc.
#   * 'rpm' ~ RedHat based, CentOS, Fedora etc.
#
# Having this information upfront, before the EC2 instance is startes from the base AMI,
# allows us to build & upload only the respective archive to the AWS instance, therefore
# making the build of new AMI much faster.
#
liferay.workspace.ee.aws.ami.base.ami.linux.packages.format=deb

# The OS user which is set up for SSH access using the default AWS key-pair
# when new EC2 instance is created base on the base AMI.
#
# "For Amazon Linux, the user name is ec2-user. For RHEL5, the user name is either root or ec2-user.
# For Ubuntu, the user name is ubuntu. For Fedora, the user name is either fedora or ec2-user.
# For SUSE Linux, the user name is either root or ec2-user. Otherwise, if ec2-user and root don't work,
# check with your AMI provider."
#   * source: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html
#
liferay.workspace.ee.aws.ami.base.ami.ssh.user.name=ubuntu

# The type of EC2 instance to use when building the AMI with Liferay Bundle installed.
# The stronger the instance, the faster the AMI will be build, but the more it will
# cost you. However, since the EC2 instance is only needed to install DEB package, therefore
# not requiring too much CPU / RAM, it's usually enough to choose even tiny instance type,
# like t2.micro.
#
# The type should be in accordance to the 'base.ami.id' you are using -- not all
# instance types support all virtualization types avaialable in AWS:
#   * t1, m1, c1, m2 (= older generations)
#       * supports only PV virtualization, so you need to choose PV-based AMI
#           if you want to use one of thse these
#   * other (= newer generations)
#       * supports only HVM virtualization
#
liferay.workspace.ee.aws.ami.build.ec2.instance.type=t2.micro

# If you want the temporary EC2 instance for building AMI launched in non-default VPC,
# provide both 'vpc.id' and 'subnet.id'. Note that your account may not have default VPC
# at all, or only in some of the regions, see these resources relating to default VPCs
# to see if your AWS account supports / has them:
#   * http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/default-vpc.html#detecting-platform
#	* http://serverfault.com/a/523484
#
# If you have default VPC available, you can leave these two properties empty.
#
liferay.workspace.ee.aws.ami.build.ec2.vpc.id=
liferay.workspace.ee.aws.ami.build.ec2.subnet.id=

# The name of the IAM profile which should be used for the EC2 instance where
# the AMI contents will be build (Liferay bundle installed, any extra provisioners
# executed). This will end up as 'iam_instance_profile' of the 'amazon-ebs' builder
# in Packer, which is producing the AMI.
#
# This might be useful if you for example have a custom provisioner (see the property
# 'liferay.workspace.ee.aws.ami.packer.extra.provisioners.json.files') which needs
# some special AWS permissions to perform its job - e.g. be able to fetch a file
# from S3 bucket which is not publicly readable.
#
# This typically matches the name of the role, assuming you've created it using AWS
# console. For details, please check AWS docs:
# https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html
#
liferay.workspace.ee.aws.ami.build.iam.instance.profile=

# The String which is put into the create AMI's name to allows AWS to 'detect'
# the Platform metadata value. This is useful since AWS console will suggest
# the SSH username to use to connect to the EC2 instances (created from this AMI)
# when the 'Connect' dialog is opened.
#
# Known keys (and their likely detected 'Platform's) are:
#   Key                 | Platform
#   'ubuntu'            -> Ubuntu
#   'amzn'              -> Amazon Linux
#   'centos'            -> Cent OS
#   'redhat' / 'RHEL'   -> Red Hat
#
liferay.workspace.ee.aws.ami.platform.key=ubuntu

# Comma-separated list of AWS regions, where the built project AMIs will be copied into,
# after the primary AMI was successfully built in the primary region (see
# 'awsAmiPrimaryRegion' above).
#
# You can also copy created AMIs into other regions manually, using AWS console:
#   * https://aws.amazon.com/blogs/aws/ec2-ami-copy-between-regions/
#
liferay.workspace.ee.aws.ami.copy.to.regions=
#liferay.workspace.ee.aws.ami.copy.to.regions=us-west-1,us-east-1

# If specified, the contents of the referenced files will be added into the Packer
# template baking the AMI in AWS - into 'provisioners' section. This can be used
# to perform additional tasks in the OS after installing Liferay bundle, producing
# AMI with some additional functionality. For example, installing some
# project-specific OS packages, creating additional configuration files etc.
#
# The referenced files must be:
#   1. a valid JSON files and
#   2. having JSON array at the top.
#
# The contents of the array should be the individual provisioners as allowed by Packer.
# The path has to be relative and will be evaluated starting from the project's
# root directory.
#
# NOTES & TIPS:
#   * Only use this if you know what you are doing.
#   * The order of the inclusion does not have to be according to the order of
#       the property values => do not assume one privisioner will be run before
#       another one
#   * Keep your extras simple - ideally only execute short inline scripts.
#   * If you need to do something more complex in every AWS AMI built, consider
#       building custom AMI (manually / automatically) and using it as the base
#       for the project
#   * You can only add to the template - existing provisioners cannot be altered or removed.
#   * You can use Packer user variables listed in 'gradle/liferay-workspace-ee/aws/packer-template.groovy-template.json'
#   * the working directory when Packer is executed will be 'gradle/liferay-workspace-ee/aws'
#       (in case you want to reference some files from inside the template)
#
liferay.workspace.ee.aws.ami.packer.extra.provisioners.json.files=

# See gradle/liferay-workspace-ee/samples/aws-ami/inline-shell-sample.json for example well-formed file
#liferay.workspace.ee.aws.ami.packer.extra.provisioners.json.file=\
#  aws-ami/extra-packer-provisioner-1.json,\
#  aws-ami/extra-packer-provisioner-2.json


# The number of AMIs (the newest ones) to retain in AWS. The older ones will be
# removed if the cleanup tasks 'removeAmisBeyondRetentionCount' is invoked.
#
liferay.workspace.ee.aws.ami.built.amis.retention.count=5


# The name of the OS service created by .deb / .rpm package after installed
# into OS.
#
# TODO rename to 'liferay' instead?
#
liferay.workspace.ee.ospackage.liferay.service.name=liferay-tomcat

# TODO add support for systemd; force users to choose or try to detect?
#liferay.workspace.ee.ospackage.init.system=sysv


# When building the ospackage (.deb / .rpm), how is the JDK being installed?
# Possible values are:
#   A) oracle-jdk:bundled
#       * Oracle JDK archive is downloaded (and cached) when the workspace is building
#       * see 'liferay.workspace.ee.ospackage.oracle.jdk.*' for details what gets fetched and installed
#       + the build will fail if the configured JDK is not available (like old Oracle JDK
#           not available for download any more)
#       - the resulting archive (.deb / .rpm) is bigger (by approx. 200 MiB)
#   B) oracle-jdk:downloaded-on-install
#       * the Oracle JDK archive is downloaded during installation of Liferay from the package (not in build-time)
#       * see 'liferay.workspace.ee.ospackage.oracle.jdk.*' for details what gets fetched and installed
#       + the resulting archive (.deb / .rpm) is smaller (by approx. 200 MiB)
#       - the build will succeed even if the configured JDK is not available any more
#           (like old Oracle JDK not available for download any more), but the installation
#           of Liferay will subsequently fail
#   C) unmanaged
#       * it's expected that JDK is already installed or will be installed before
#           Liferay is started for the first time
#       * when this option is used, 'liferay.workspace.ee.ospackage.jdk.home' is mandatory
#           * use empty value if 'java' will be available in PATH
#       * you can use 'liferay.workspace.ee.ospackage.extra.pre.install' (or '*.post.install')
#           to execute your script installing the JDK or you can e.g. install in the in AMI
#           you use as base image
#
liferay.workspace.ee.ospackage.jdk.installation.type=oracle-jdk:bundled

#liferay.workspace.ee.ospackage.jdk.installation.type=oracle-jdk:downloaded-on-install
#liferay.workspace.ee.ospackage.jdk.installation.type=unmanaged

# The path where JDK can be found if not managed by ospackage in the workspace ee.
# This only has effect when 'liferay.workspace.ee.ospackage.jdk.installation.type=unmanaged'.
#
# Empty (no value) means the JDK is available in PATH.
#
# Special value '_not_set_' is used by default to determine that user did not set
# any custom value, because empty is not the same as not set, see above.
#
liferay.workspace.ee.ospackage.jdk.home=_not_set_

#liferay.workspace.ee.ospackage.jdk.home=

# The details for downloading Oracle JDK 7. Used when 'liferay.workspace.ee.java.version.major=7' and
# 'liferay.workspace.ee.ospackage.jdk.installation.type=oracle-jdk-*'
#
# NOTE: This will be an UNSTABLE link (url), since JDK 7 is out of support
# and oracle provides the archive only with oracle.com credentials
#
liferay.workspace.ee.ospackage.oracle.jdk.7.version=7u79
liferay.workspace.ee.ospackage.oracle.jdk.7.url=https://download.oracle.com/otn/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz
liferay.workspace.ee.ospackage.oracle.jdk.7.archive.name=jdk-7u79-linux-x64.tar.gz
liferay.workspace.ee.ospackage.oracle.jdk.7.top.level.dir=jdk1.7.0_79
liferay.workspace.ee.ospackage.oracle.jdk.7.md5=9222e097e624800fdd9bfb568169ccad
liferay.workspace.ee.ospackage.oracle.jdk.7.sha256=29d75d0022bfa211867b876ddd31a271b551fa10727401398295e6e666a11d90

# The details for downloading Oracle JDK 8. Used when 'liferay.workspace.ee.java.version.major=7' and
# 'liferay.workspace.ee.ospackage.jdk.installation.type=oracle-jdk-*'
#
# If there is a newer JDK 8 (http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html),
# you can used it by overriding the following properties.
#
# NOTE: This will be an UNSTABLE link (url), since JDK 8 is getting new releases and only
# the very latest can be downloaded publicly from oracle.com; for older releases,
# oracle.com credentials (username + password) are required; sometime a release may
# disappear (404 on the URL), because a new build for that release was created
#
# Oracle switched to HTTP most likely because of SSL certificate
# mismatch (a248.e.akamai.net does not match the download.oracle.com domain)
#
liferay.workspace.ee.ospackage.oracle.jdk.8.version=8u171
liferay.workspace.ee.ospackage.oracle.jdk.8.url=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz
liferay.workspace.ee.ospackage.oracle.jdk.8.archive.name=jdk-8u171-linux-x64.tar.gz
liferay.workspace.ee.ospackage.oracle.jdk.8.top.level.dir=jdk1.8.0_171
liferay.workspace.ee.ospackage.oracle.jdk.8.md5=43dafc862dd98bcff889e1239625e7a3
liferay.workspace.ee.ospackage.oracle.jdk.8.sha256=b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982

# Every property of 'liferay.workspace.ee.ospackage.extra.*' will be interpreted
# as a list of files (relative to project root), which will be run at given phase
# when .deb / .rpm is being installed or removed from OS.
#
# Given paths must exist in the project when used. Scripts will be executed using
# root privileges when installing .deb or .rpm into the OS.
#
# NOTE: The script will be executed with shebang '#!/bin/sh -e', so all individual
# commands must return 0. As a rule of thumb, do hot put shebang into your scripts,
# as it will either be stripped (therefore having no effect) or cause build failure.
#
liferay.workspace.ee.ospackage.extra.pre.install=
liferay.workspace.ee.ospackage.extra.post.install=
liferay.workspace.ee.ospackage.extra.pre.uninstall=
liferay.workspace.ee.ospackage.extra.post.uninstall=

# The list of packages which your .deb will depend on. This is useful if in your
# custom 'ospackage.extra.*' scripts use something that might not be always
# available, or e.g. you want to require that 'java' is already present. If you define
# dependency, the package manager in your OS will require you to install
# it before you are able to install Liferay's package you are building.
#
# NOTE: You cannot install packages from within the pre/post scripts.
#       For example, when building a .deb, you cannot have a 'pre.install' script
#       which calls 'apt-get'. You may be able to use something like 'aptdaemon'
#       and queue your package installation with 'aptdcon' command for later installation,
#       but it's problematic and not recommended. Instead, use for example
#       'liferay.workspace.ee.aws.ami.packer.extra.provisioners.json.files' to install
#       what's necessary.
#
# NOTE: 'tar' will always be required, since we use it to extract built Liferay bundle.
#
liferay.workspace.ee.ospackage.extra.required.packages=

#liferay.workspace.ee.ospackage.extra.required.packages=openjdk-8-jdk


##
## Configuration for building Dockerfiles
##      (e.g. 'gradlew distBundleDockerfile' or 'gradlew distBundleDockerImageLocal')
##
## Docker will use the .deb file built by ospackage module, so you get all the content
## and features included in there (OS user and group, selected JDK installed etc.)
##

# The first part of the Docker repository name to be used in when tagging built
# Docker images. The second part is added automatically based on the project's name.
#
# Example: if you specify value 'acme' and your Liferay Workspace is named 'liferay-portal'
# (setting.gradle in the Gradle project) ), the Docker repository will be named
# 'acme/liferay-portal'
#
liferay.workspace.ee.docker.repository.company=acme

# The value of the MAINTAINER directive put into Dockerfile. It should contain
# a valid name and email of the maintainer, in the form 'name <email>'.
#
# No validation or transformation is performed on the value.
#
liferay.workspace.ee.docker.maintainer=Docker Maintainer <docker-maintainer@yourdomain.com>

# The value of the EXPOSE directive put into Dockerfile, determining which ports
# will the Docker container open to the outer world. Based on Docker documentation,
# this is space-separated list of port numbers.
#
# No validation or transformation is performed on the value.
#
# This has to be in sync with your your app server configuration in '/configs/**'
# (e.g. Tomcat) -- you will want to open all used HTTP / AJP ports, as well as
# for example JMX (typically on 9000) if you want to use if from outside of the
# Docker container.
#
liferay.workspace.ee.docker.expose=8080 8081

# The set of Packer extra provisioners to be used when building a Docker image.
#
# Same principle as liferay.workspace.ee.aws.ami.packer.extra.provisioners.json.files,
# see its comments for further details.
#
liferay.workspace.ee.docker.packer.extra.provisioners.json.files=